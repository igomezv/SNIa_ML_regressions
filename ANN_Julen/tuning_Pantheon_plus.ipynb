{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 17:18:58.300799: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-03 17:18:58.412189: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-03 17:18:58.412206: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-03 17:18:59.026176: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-03 17:18:59.026229: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-03 17:18:59.026236: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from astroNN.nn.layers import MCDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'https://raw.githubusercontent.com/igomezv/SNIa_ML_regressions/main/data/pantheon%2B_lcparam_full_long_zhel.txt'\n",
    "syscov_file = 'https://raw.githubusercontent.com/igomezv/SNIa_ML_regressions/main/data/pantheon%2B_sys_full_long.txt'\n",
    "# file = 'data/lcparam_full_long.txt'\n",
    "df = pd.read_csv(file, sep = \" \", usecols=['zHD', 'm_b_corr'])\n",
    "# df = pd.read_csv(file, sep = \" \")\n",
    "N = len(df.values)\n",
    "syscov = np.loadtxt(syscov_file, skiprows=1).reshape((N, N))\n",
    "cov = np.copy(syscov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zHD</th>\n",
       "      <th>m_b_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00122</td>\n",
       "      <td>9.74571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00122</td>\n",
       "      <td>9.80286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00256</td>\n",
       "      <td>11.47030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00256</td>\n",
       "      <td>11.49190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00299</td>\n",
       "      <td>11.52270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       zHD  m_b_corr\n",
       "0  0.00122   9.74571\n",
       "1  0.00122   9.80286\n",
       "2  0.00256  11.47030\n",
       "3  0.00256  11.49190\n",
       "4  0.00299  11.52270"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    1,    2, ..., 1698, 1699, 1700]),\n",
       " array([   0,    1,    2, ..., 1698, 1699, 1700]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag_indices_from(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_errors = np.diag(syscov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zHD</th>\n",
       "      <th>m_b_corr</th>\n",
       "      <th>errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1701.000000</td>\n",
       "      <td>1701.000000</td>\n",
       "      <td>1701.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.221229</td>\n",
       "      <td>19.083629</td>\n",
       "      <td>0.169411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.249271</td>\n",
       "      <td>3.374499</td>\n",
       "      <td>0.059434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001220</td>\n",
       "      <td>9.745710</td>\n",
       "      <td>0.080278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.027730</td>\n",
       "      <td>16.036200</td>\n",
       "      <td>0.128216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.163750</td>\n",
       "      <td>20.076300</td>\n",
       "      <td>0.155560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.328680</td>\n",
       "      <td>21.824800</td>\n",
       "      <td>0.195335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.261370</td>\n",
       "      <td>26.929800</td>\n",
       "      <td>0.625326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               zHD     m_b_corr       errors\n",
       "count  1701.000000  1701.000000  1701.000000\n",
       "mean      0.221229    19.083629     0.169411\n",
       "std       0.249271     3.374499     0.059434\n",
       "min       0.001220     9.745710     0.080278\n",
       "25%       0.027730    16.036200     0.128216\n",
       "50%       0.163750    20.076300     0.155560\n",
       "75%       0.328680    21.824800     0.195335\n",
       "max       2.261370    26.929800     0.625326"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['errors'] = np.sqrt(sq_errors+dmag**2)\n",
    "df['errors'] = np.sqrt(sq_errors)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomize = np.random.permutation(N)\n",
    "data = df.values[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = data[:,0]\n",
    "y = data[:,1:]\n",
    "# print(randomize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerz = StandardScaler()\n",
    "# scalerz = MinMaxScaler(feature_range=(-1,1))\n",
    "# fit scaler on data\n",
    "scalerz.fit(z.reshape(-1,1))\n",
    "# apply transform\n",
    "z = scalerz.transform(z.reshape(-1,1))\n",
    "\n",
    "# scalery = MinMaxScaler(feature_range=(0,1))\n",
    "# scalery = StandardScaler()\n",
    "# scalery.fit(y)\n",
    "# y = scalery.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.8\n",
    "ntrain = int(split * len(z))\n",
    "indx = [ntrain]\n",
    "z_train, z_test = np.split(z, indx)\n",
    "y_train, y_test = np.split(y, indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',\n",
    "                                   min_delta=0.0,\n",
    "                                   patience=50.0,\n",
    "                                   restore_best_weights=True)]\n",
    "#                      tf.keras.callbacks.ReduceLROnPlateau(patience=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS1 = hp.HParam('num_units1', hp.Discrete([50, 100, 150, 200]))\n",
    "HP_NUM_UNITS2 = hp.HParam('num_units2', hp.Discrete([50, 100, 150, 200]))\n",
    "HP_NUM_UNITS3 = hp.HParam('num_units3', hp.Discrete([50, 100, 150, 200]))\n",
    "# HP_NUM_UNITS4 = hp.HParam('num_units4', hp.Discrete([50, 100, 150, 200]))\n",
    "# HP_NUM_UNITS3 = hp.HParam('num_units3', hp.Discrete([50, 100, 150, 200]))\n",
    "# HP_NUM_UNITS4 = hp.HParam('num_units4', hp.Discrete([2, 5, 10]))\n",
    "# HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.0, 0.2))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'Adadelta']))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
    "HP_BATCHSIZE = hp.HParam('batch_size', hp.Discrete([4, 8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 17:19:00.727986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-03 17:19:00.728175: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-03 17:19:00.728230: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-04-03 17:19:00.728277: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-04-03 17:19:00.728324: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-04-03 17:19:00.728369: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-04-03 17:19:00.728415: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-04-03 17:19:00.728466: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-04-03 17:19:00.728514: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-04-03 17:19:00.728522: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-04-03 17:19:00.728716: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "# with tf.summary.FileWriter('logs/hparam_tuning', sess.graph):\n",
    "#     init = tf.initialize_all_variables()\n",
    "#     sess.run(init)\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_NUM_UNITS1, HP_NUM_UNITS2, HP_NUM_UNITS3, HP_BATCHSIZE],\n",
    "        metrics=[hp.Metric('loss', display_name=\"Loss\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_regression_dropout(input_z, hparams):\n",
    "    # Defeine Keras model for regression\n",
    "    efirst = Dense(hparams[HP_NUM_UNITS1], activation='relu', input_shape=(1,))(input_z)\n",
    "#     model = tf.keras.models.Sequential()\n",
    "#     model.add(tf.keras.layers.InputLayer(batch_input_shape=((None, 1))))\n",
    "    x = Dense(hparams[HP_NUM_UNITS2], activation='relu')(efirst)\n",
    "#     model.add(Dense(units=hparams[HP_NUM_UNITS1], activation='relu'))\n",
    "    x = MCDropout(0.3)(x)\n",
    "    x = Dense(hparams[HP_NUM_UNITS3], activation='relu')(x)\n",
    "    x = MCDropout(0.3)(x)\n",
    "#     x = Dense(hparams[HP_NUM_UNITS4], activation='relu')(x)\n",
    "#     x = MCDropout(0.3)(x)\n",
    "#     ehidden2 = Dense(hparams[HP_NUM_UNITS3], activation='relu')(ehidden1)\n",
    "    elast = Dense(2, activation='linear')(x)\n",
    "        \n",
    "    return elast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "input_z = tf.keras.layers.Input(shape = (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "    model = Model(input_z, model_regression_dropout(input_z, hparams))  \n",
    "#     model = Model(input_z, hzmodel(input_z, hparams))  \n",
    "                              \n",
    "    model.compile(\n",
    "#             optimizer=hparams[HP_OPTIMIZER],\n",
    "            optimizer='adam',\n",
    "            loss='mean_squared_error',\n",
    "            metrics=['mean_squared_error']\n",
    "          )\n",
    "    # Run with 1 epoch to speed things up for demo purposes\n",
    "    # model.fit(x_train, y_train, epochs=1, callbacks=callbacks) \n",
    "    # _, loss = model.evaluate(x_test, y_test)\n",
    "    model_trained = model.fit(z_train, y_train, epochs=epochs, \n",
    "                              batch_size=hparams[HP_BATCHSIZE], verbose=0) \n",
    "    \n",
    "    loss = model_trained.history['mean_squared_error'][-1]\n",
    "\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "#     with tf.summary.FileWriter(run_dir):\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(\"loss\", loss, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'num_units1': 50, 'num_units2': 50, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-1\n",
      "{'num_units1': 50, 'num_units2': 50, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-2\n",
      "{'num_units1': 50, 'num_units2': 50, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-3\n",
      "{'num_units1': 50, 'num_units2': 50, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-4\n",
      "{'num_units1': 50, 'num_units2': 100, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-5\n",
      "{'num_units1': 50, 'num_units2': 100, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-6\n",
      "{'num_units1': 50, 'num_units2': 100, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-7\n",
      "{'num_units1': 50, 'num_units2': 100, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-8\n",
      "{'num_units1': 50, 'num_units2': 150, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-9\n",
      "{'num_units1': 50, 'num_units2': 150, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-10\n",
      "{'num_units1': 50, 'num_units2': 150, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-11\n",
      "{'num_units1': 50, 'num_units2': 150, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-12\n",
      "{'num_units1': 50, 'num_units2': 200, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-13\n",
      "{'num_units1': 50, 'num_units2': 200, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-14\n",
      "{'num_units1': 50, 'num_units2': 200, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-15\n",
      "{'num_units1': 50, 'num_units2': 200, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-16\n",
      "{'num_units1': 100, 'num_units2': 50, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-17\n",
      "{'num_units1': 100, 'num_units2': 50, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-18\n",
      "{'num_units1': 100, 'num_units2': 50, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-19\n",
      "{'num_units1': 100, 'num_units2': 50, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-20\n",
      "{'num_units1': 100, 'num_units2': 100, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-21\n",
      "{'num_units1': 100, 'num_units2': 100, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-22\n",
      "{'num_units1': 100, 'num_units2': 100, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-23\n",
      "{'num_units1': 100, 'num_units2': 100, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-24\n",
      "{'num_units1': 100, 'num_units2': 150, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-25\n",
      "{'num_units1': 100, 'num_units2': 150, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-26\n",
      "{'num_units1': 100, 'num_units2': 150, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-27\n",
      "{'num_units1': 100, 'num_units2': 150, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-28\n",
      "{'num_units1': 100, 'num_units2': 200, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-29\n",
      "{'num_units1': 100, 'num_units2': 200, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-30\n",
      "{'num_units1': 100, 'num_units2': 200, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-31\n",
      "{'num_units1': 100, 'num_units2': 200, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-32\n",
      "{'num_units1': 150, 'num_units2': 50, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-33\n",
      "{'num_units1': 150, 'num_units2': 50, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-34\n",
      "{'num_units1': 150, 'num_units2': 50, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-35\n",
      "{'num_units1': 150, 'num_units2': 50, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-36\n",
      "{'num_units1': 150, 'num_units2': 100, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-37\n",
      "{'num_units1': 150, 'num_units2': 100, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-38\n",
      "{'num_units1': 150, 'num_units2': 100, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-39\n",
      "{'num_units1': 150, 'num_units2': 100, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-40\n",
      "{'num_units1': 150, 'num_units2': 150, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-41\n",
      "{'num_units1': 150, 'num_units2': 150, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-42\n",
      "{'num_units1': 150, 'num_units2': 150, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-43\n",
      "{'num_units1': 150, 'num_units2': 150, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-44\n",
      "{'num_units1': 150, 'num_units2': 200, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-45\n",
      "{'num_units1': 150, 'num_units2': 200, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-46\n",
      "{'num_units1': 150, 'num_units2': 200, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-47\n",
      "{'num_units1': 150, 'num_units2': 200, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-48\n",
      "{'num_units1': 200, 'num_units2': 50, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-49\n",
      "{'num_units1': 200, 'num_units2': 50, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-50\n",
      "{'num_units1': 200, 'num_units2': 50, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-51\n",
      "{'num_units1': 200, 'num_units2': 50, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-52\n",
      "{'num_units1': 200, 'num_units2': 100, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-53\n",
      "{'num_units1': 200, 'num_units2': 100, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-54\n",
      "{'num_units1': 200, 'num_units2': 100, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-55\n",
      "{'num_units1': 200, 'num_units2': 100, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-56\n",
      "{'num_units1': 200, 'num_units2': 150, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-57\n",
      "{'num_units1': 200, 'num_units2': 150, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-58\n",
      "{'num_units1': 200, 'num_units2': 150, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-59\n",
      "{'num_units1': 200, 'num_units2': 150, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-60\n",
      "{'num_units1': 200, 'num_units2': 200, 'num_units3': 50, 'batch_size': 4}\n",
      "--- Starting trial: run-61\n",
      "{'num_units1': 200, 'num_units2': 200, 'num_units3': 100, 'batch_size': 4}\n",
      "--- Starting trial: run-62\n",
      "{'num_units1': 200, 'num_units2': 200, 'num_units3': 150, 'batch_size': 4}\n",
      "--- Starting trial: run-63\n",
      "{'num_units1': 200, 'num_units2': 200, 'num_units3': 200, 'batch_size': 4}\n",
      "--- Starting trial: run-64\n",
      "{'num_units1': 50, 'num_units2': 50, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-65\n",
      "{'num_units1': 50, 'num_units2': 50, 'num_units3': 100, 'batch_size': 8}\n",
      "--- Starting trial: run-66\n",
      "{'num_units1': 50, 'num_units2': 50, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-67\n",
      "{'num_units1': 50, 'num_units2': 50, 'num_units3': 200, 'batch_size': 8}\n",
      "--- Starting trial: run-68\n",
      "{'num_units1': 50, 'num_units2': 100, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-69\n",
      "{'num_units1': 50, 'num_units2': 100, 'num_units3': 100, 'batch_size': 8}\n",
      "--- Starting trial: run-70\n",
      "{'num_units1': 50, 'num_units2': 100, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-71\n",
      "{'num_units1': 50, 'num_units2': 100, 'num_units3': 200, 'batch_size': 8}\n",
      "--- Starting trial: run-72\n",
      "{'num_units1': 50, 'num_units2': 150, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-73\n",
      "{'num_units1': 50, 'num_units2': 150, 'num_units3': 100, 'batch_size': 8}\n",
      "--- Starting trial: run-74\n",
      "{'num_units1': 50, 'num_units2': 150, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-75\n",
      "{'num_units1': 50, 'num_units2': 150, 'num_units3': 200, 'batch_size': 8}\n",
      "--- Starting trial: run-76\n",
      "{'num_units1': 50, 'num_units2': 200, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-77\n",
      "{'num_units1': 50, 'num_units2': 200, 'num_units3': 100, 'batch_size': 8}\n",
      "--- Starting trial: run-78\n",
      "{'num_units1': 50, 'num_units2': 200, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-79\n",
      "{'num_units1': 50, 'num_units2': 200, 'num_units3': 200, 'batch_size': 8}\n",
      "--- Starting trial: run-80\n",
      "{'num_units1': 100, 'num_units2': 50, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-81\n",
      "{'num_units1': 100, 'num_units2': 50, 'num_units3': 100, 'batch_size': 8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-82\n",
      "{'num_units1': 100, 'num_units2': 50, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-83\n",
      "{'num_units1': 100, 'num_units2': 50, 'num_units3': 200, 'batch_size': 8}\n",
      "--- Starting trial: run-84\n",
      "{'num_units1': 100, 'num_units2': 100, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-85\n",
      "{'num_units1': 100, 'num_units2': 100, 'num_units3': 100, 'batch_size': 8}\n",
      "--- Starting trial: run-86\n",
      "{'num_units1': 100, 'num_units2': 100, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-87\n",
      "{'num_units1': 100, 'num_units2': 100, 'num_units3': 200, 'batch_size': 8}\n",
      "--- Starting trial: run-88\n",
      "{'num_units1': 100, 'num_units2': 150, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-89\n",
      "{'num_units1': 100, 'num_units2': 150, 'num_units3': 100, 'batch_size': 8}\n",
      "--- Starting trial: run-90\n",
      "{'num_units1': 100, 'num_units2': 150, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-91\n",
      "{'num_units1': 100, 'num_units2': 150, 'num_units3': 200, 'batch_size': 8}\n",
      "--- Starting trial: run-92\n",
      "{'num_units1': 100, 'num_units2': 200, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-93\n",
      "{'num_units1': 100, 'num_units2': 200, 'num_units3': 100, 'batch_size': 8}\n",
      "--- Starting trial: run-94\n",
      "{'num_units1': 100, 'num_units2': 200, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-95\n",
      "{'num_units1': 100, 'num_units2': 200, 'num_units3': 200, 'batch_size': 8}\n",
      "--- Starting trial: run-96\n",
      "{'num_units1': 150, 'num_units2': 50, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-97\n",
      "{'num_units1': 150, 'num_units2': 50, 'num_units3': 100, 'batch_size': 8}\n",
      "--- Starting trial: run-98\n",
      "{'num_units1': 150, 'num_units2': 50, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-99\n",
      "{'num_units1': 150, 'num_units2': 50, 'num_units3': 200, 'batch_size': 8}\n",
      "--- Starting trial: run-100\n",
      "{'num_units1': 150, 'num_units2': 100, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-101\n",
      "{'num_units1': 150, 'num_units2': 100, 'num_units3': 100, 'batch_size': 8}\n",
      "--- Starting trial: run-102\n",
      "{'num_units1': 150, 'num_units2': 100, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-103\n",
      "{'num_units1': 150, 'num_units2': 100, 'num_units3': 200, 'batch_size': 8}\n",
      "--- Starting trial: run-104\n",
      "{'num_units1': 150, 'num_units2': 150, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-105\n",
      "{'num_units1': 150, 'num_units2': 150, 'num_units3': 100, 'batch_size': 8}\n",
      "--- Starting trial: run-106\n",
      "{'num_units1': 150, 'num_units2': 150, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-107\n",
      "{'num_units1': 150, 'num_units2': 150, 'num_units3': 200, 'batch_size': 8}\n",
      "--- Starting trial: run-108\n",
      "{'num_units1': 150, 'num_units2': 200, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-109\n",
      "{'num_units1': 150, 'num_units2': 200, 'num_units3': 100, 'batch_size': 8}\n",
      "--- Starting trial: run-110\n",
      "{'num_units1': 150, 'num_units2': 200, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-111\n",
      "{'num_units1': 150, 'num_units2': 200, 'num_units3': 200, 'batch_size': 8}\n",
      "--- Starting trial: run-112\n",
      "{'num_units1': 200, 'num_units2': 50, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-113\n",
      "{'num_units1': 200, 'num_units2': 50, 'num_units3': 100, 'batch_size': 8}\n",
      "--- Starting trial: run-114\n",
      "{'num_units1': 200, 'num_units2': 50, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-115\n",
      "{'num_units1': 200, 'num_units2': 50, 'num_units3': 200, 'batch_size': 8}\n",
      "--- Starting trial: run-116\n",
      "{'num_units1': 200, 'num_units2': 100, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-117\n",
      "{'num_units1': 200, 'num_units2': 100, 'num_units3': 100, 'batch_size': 8}\n",
      "--- Starting trial: run-118\n",
      "{'num_units1': 200, 'num_units2': 100, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-119\n",
      "{'num_units1': 200, 'num_units2': 100, 'num_units3': 200, 'batch_size': 8}\n",
      "--- Starting trial: run-120\n",
      "{'num_units1': 200, 'num_units2': 150, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-121\n",
      "{'num_units1': 200, 'num_units2': 150, 'num_units3': 100, 'batch_size': 8}\n",
      "--- Starting trial: run-122\n",
      "{'num_units1': 200, 'num_units2': 150, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-123\n",
      "{'num_units1': 200, 'num_units2': 150, 'num_units3': 200, 'batch_size': 8}\n",
      "--- Starting trial: run-124\n",
      "{'num_units1': 200, 'num_units2': 200, 'num_units3': 50, 'batch_size': 8}\n",
      "--- Starting trial: run-125\n",
      "{'num_units1': 200, 'num_units2': 200, 'num_units3': 100, 'batch_size': 8}\n",
      "--- Starting trial: run-126\n",
      "{'num_units1': 200, 'num_units2': 200, 'num_units3': 150, 'batch_size': 8}\n",
      "--- Starting trial: run-127\n",
      "{'num_units1': 200, 'num_units2': 200, 'num_units3': 200, 'batch_size': 8}\n",
      "Total combinations: 129\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "for batch_size in HP_BATCHSIZE.domain.values:\n",
    "    for num_units1 in HP_NUM_UNITS1.domain.values:\n",
    "        for num_units2 in HP_NUM_UNITS2.domain.values:\n",
    "            for num_units3 in HP_NUM_UNITS3.domain.values:\n",
    "#                 for num_units4 in HP_NUM_UNITS4.domain.values:\n",
    "#       for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "#                     for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                hparams = {\n",
    "                  HP_NUM_UNITS1: num_units1,\n",
    "                  HP_NUM_UNITS2: num_units2,\n",
    "                  HP_NUM_UNITS3: num_units3,\n",
    "#                       HP_NUM_UNITS4: num_units4,\n",
    "                #                       HP_DROPOUT: dropout_rate,\n",
    "    #                       HP_OPTIMIZER: optimizer,\n",
    "                  HP_BATCHSIZE: batch_size\n",
    "                }\n",
    "                run_name = \"run-%d\" % session_num\n",
    "                print('--- Starting trial: %s' % run_name)\n",
    "                print({h.name: hparams[h] for h in hparams})\n",
    "                run('logs/hparam_tuning/' + run_name, hparams)\n",
    "                session_num += 1\n",
    "\n",
    "print(\"Total combinations: {}\".format(session_num+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: kill: (7439) - No such process\r\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1419b4d63b8f0c40\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1419b4d63b8f0c40\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!kill 7439\n",
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
